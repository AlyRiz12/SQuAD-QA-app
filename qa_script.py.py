# -*- coding: utf-8 -*-
"""Salinan dari Sistem QA Metode BERT_Kelompok 11 coba deploy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PrD387IPj2jHPKLZpwQe5di5dYSJuLgY

**Kelompok 11 Data dan Analisis**

- Alyssa Rizanty (1519622005)
- Nadya Faiza (1519622003)
- Nur Assyiah (1519622004)

# **Implementasi Sistem Question Answering Menggunakan metode BERT**

**BERT** merupakan singkatan dari Bidirectional Encoder Representations from Transformers.

Mudahnya, **BERT** adalah model AI dari Google yang sangat pintar untuk memahami teks.

**Penjelasan Dataset**

SQuAD (Stanford Question Answering Dataset) adalah kumpulan data besar yang dirancang untuk melatih dan mengevaluasi sistem tanya jawab. Kumpulan data ini terdiri dari pertanyaan-pertanyaan yang diajukan oleh banyak orang pada artikel Wikipedia, dengan jawaban yang spesifik dalam bentuk rentang teks dari artikel-artikel tersebut.

Dataset ini berisi 6 kolom. Tetapi yang digunakan dalam project ini hanya 3 kolom saja, yaitu kolom context, question, dan text yang nantinya diubah menjadi kolom answer.

**Penjelasan Setiap Kolom**

- Unnamed: 0 : nomor urut
- context : potongan teks atau paragraf (berupa artikel atau kutipan berita)
- quetion : pertanyaan yang jawabannya bisa ditemukan di context itu
- id : nomor kode dari pertanyaan
- answer_start : posisi (angka index) dimana jawaban mulai muncul di context
- text : jawaban untuk pertanyaan, diambil dari context

**Tujuan Project**

Mengembangkan sebuah sistem Question Answering (QA) berbasis teks yang mampu menjawab pertanyaan pengguna dengan akurat, melalui serangkaian tahapan mulai dari pemrosesan data, evaluasi beberapa model, hingga deployment berbasis Command Line Interface (CLI)

## **IMPORT LIBRARY**
"""

# Bersihkan dan install transformers stabil
!pip uninstall -y transformers
!pip install transformers==4.37.2

"""**Penjelasan:**

Kode ini digunakan untuk membersihkan versi lama/bermasalah dari transformers dan install versi yang lebih stabil, yaitu versi 4.37.2
"""

# Core Libraries
import re  # Untuk pemrosesan teks (regex)
import warnings  # Untuk menghilangkan peringatan
import numpy as np  # Operasi numerik
import pandas as pd  # Manipulasi data

# Visualization Libraries
import matplotlib.pyplot as plt  # Membuat grafik
import seaborn as sns  # Visualisasi data

# Machine Learning & Deep Learning
import torch  # PyTorch untuk deep learning

# Transformers (Hugging Face)
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline

# Dataset and Evaluation
from sklearn.model_selection import train_test_split  # Membagi data latih dan uji

# Settings
warnings.filterwarnings("ignore")  # Supaya output bersih dari warning

"""**Penjelasan:**

Kode ini mengimpor library penting untuk membangun program Question Answering (QA) berbasis machine learning.

- Library seperti re, warnings, numpy, dan pandas digunakan untuk mengolah data.

- matplotlib dan seaborn dipakai untuk membuat visualisasi.

- Untuk machine learning, digunakan PyTorch (torch) dan Hugging Face transformers agar bisa memanfaatkan model seperti BERT.

- Data dibagi menjadi data latih dan data uji menggunakan sklearn.

- Peringatan dihilangkan supaya output lebih rapi.

## **IMPORT DATASET**
"""

# hubungkan colab ke google drive
from google.colab import drive
drive.mount('/content/drive')

# import dataset
df = pd.read_csv('/content/drive/MyDrive/SQuAD/SQuAD_csv.csv')

"""**Penjelasan:**

Kode ini berfungsi untuk membaca file CSV bernama SQuAD_csv.csv yang disimpan di Google Drive, lalu menyimpannya ke dalam variabel df dalam bentuk DataFrame menggunakan library pandas. Dengan ini, data bisa diolah, dianalisis, atau dimanipulasi di Google Colab.

## **MEMBACA DATASET**
"""

# Baca 5 baris awal dataset
df.head()

"""**Penjelasan:**

Kode menampilkan lima baris pertama dari dataset untuk memberikan gambaran awal mengenai struktur dan isi data.
"""

# Lihat info dataset
df.info()

"""**Penjelasan:**

Kode ini menampilkan gambaran umum dataset, seperti jumlah baris, kolom, tipe data, dan data yang kosong. Dataset ini punya 86.821 baris dan 6 kolom, dengan tipe data integer dan string. Kolom "text" punya tiga nilai kosong, sementara kolom lain lengkap. Informasi ini membantu memahami struktur dan kualitas data.
"""

# Total jumlah dataset
len(df)

"""**Penjelasan:**

Kode ini digunakan untuk menghitung jumlah total baris dalam dataset yang disimpan dalam variabel df. Dataset ini memiliki 86.821 jumlah data.

## **MEMBERSIHKAN DATASET**

**Nilai Kosong**
"""

# Periksa apakah ada nilai kosong (null) di dalam dataset
print("Data dengan Nilai Kosong:")
df.isnull().sum()

"""**Penjelasan:**

Kode ini digunakan untuk mengecek jumlah data kosong di setiap kolom. Hasilnya menunjukkan ada tiga nilai kosong di kolom "text", yang perlu ditangani lebih lanjut.
"""

# Tampilkan data yang punya nilai kosong (null)
df[df.isnull().any(axis=1)]

"""**Penjelasan:**

Kode ini digunakan untuk menampilkan semua baris yang punya data kosong di dalamnya.
"""

# Hapus data yang punya nilai kosong (null)
df = df.dropna()
missing_values = df.isnull().sum()
missing_values

"""**Penjelasan:**

Kode ini menghapus semua baris kosong dengan dropna(), lalu mengecek ulang jumlah data kosong dan menyimpannya ke missing_values. Hasilnya, DataFrame df sudah bersih dari data kosong.

**Data Duplikat**
"""

# Periksa baris duplikat di dalam dataset
duplicates = df.duplicated()
print(f"Jumlah baris duplikat: {duplicates.sum()}")

"""**Penjelasan:**

Kode ini mengecek dan menghitung baris duplikat di df dengan duplicated(). Hasilnya, tidak ada data duplikat di dataset ini.

**Hapus Kolom yang Tidak Terpakai**
"""

# Menghapus kolom yang tidak digunakan
df = df.drop(columns=['Unnamed: 0', 'id', 'answer_start'])
df.head()

"""**Penjelasan:**

Kode ini menghapus kolom "Unnamed: 0", "id", dan "answer_start" karena tidak dibutuhkan, lalu menampilkan 5 baris pertama dari DataFrame yang sudah diperbarui.

**Ganti Nama Kolom**
"""

# ganti nama kolom
df.rename(columns={'text': 'answer'}, inplace=True)
df.head()

"""**Penjelasan:**

Kode ini mengganti nama kolom "text" menjadi "answer" dan langsung menerapkan perubahan, lalu menampilkan 5 baris pertama dari DataFrame.

**Membersihkan Teks (Text Cleaning)**
"""

# Mendefinisikan fungsi untuk membersihkan teks
def clean_text(text):
    text = re.sub(r"\(.*?\)", "", text)  # Menghapus teks dalam tanda kurung
    text = re.sub(r'\s+', ' ', text.strip().lower())  # Normalisasi spasi dan ubah ke huruf kecil
    return text

# Terapkan fungsi cleaning ke kolom 'context', 'question', dan 'answer'
df['context'] = df['context'].apply(clean_text)
df['question'] = df['question'].apply(clean_text)
df['answer'] = df['answer'].apply(clean_text)

# Membersihkan lagi: huruf kecil, hapus spasi ekstra
df['context'] = df['context'].str.lower().str.strip().apply(lambda x: re.sub(r'\s+', ' ', x))
df['question'] = df['question'].str.lower().str.strip().apply(lambda x: re.sub(r'\s+', ' ', x))
df['answer'] = df['answer'].str.lower().str.strip().apply(lambda x: re.sub(r'\s+', ' ', x))

"""**Penjelasan:**

Kode ini mendefinisikan fungsi clean_text() untuk membersihkan teks di kolom "context", "question", dan "answer". Fungsi ini menghapus teks dalam tanda kurung, menormalkan spasi, dan mengubah teks menjadi huruf kecil. Proses pembersihan diterapkan ke setiap kolom, memastikan teks bebas dari spasi ekstra dan huruf kapital.

**Cek Jumlah Data Unik**
"""

# Menampilkan jumlah pertanyaan dan jawaban unik
print(f"Unique contexts: {df['context'].nunique()}")
print(f"Unique questions: {df['question'].nunique()}")
print(f"Unique answers: {df['answer'].nunique()}")

"""**Penjelasan:**

Kode ini menampilkan jumlah nilai unik di kolom "context", "question", dan "answer" untuk mengetahui variasi data dalam kolom-kolom tersebut.

Hasilnya menunjukkan jumlah nilai unik di setiap kolom sebagai berikut:

- Unique contexts: 18,877

- Unique questions: 86,724

- Unique answers: 63,796

**Tampilkan Data Akhir**
"""

# Menampilkan info dan contoh data setelah semua pembersihan
print("Info Dataset Final:")
df.info()

print("Contoh Dataset Final:")
df.head()

"""**Penjelasan:**

Kode ini menampilkan informasi dan contoh data setelah pembersihan. df.info() memberikan ringkasan tentang data, memastikan tidak ada nilai kosong, sementara df.head() menampilkan 5 baris pertama untuk memverifikasi data yang sudah dibersihkan.
"""

# Simpan dataset bersih
df_clean = df.copy()

"""## **PREPROCESSING**"""

# Split dataset menjadi train dan validation
train_texts, val_texts, train_questions, val_questions, train_answers, val_answers = train_test_split(
    df_clean['context'], df_clean['question'], df_clean['answer'], test_size=0.2, random_state=42
)

"""Penjelasan:

Kode ini membagi dataset menjadi dua bagian: 80% untuk pelatihan dan 20% untuk validasi, menggunakan train_test_split dari sklearn. Data yang dibagi meliputi context, question, dan answer. Penggunaan random_state=42 memastikan hasil pembagian tetap sama setiap dijalankan. Hasilnya, diperoleh enam variabel: tiga untuk data pelatihan dan tiga untuk data validasi, yang digunakan untuk melatih dan menguji model.
"""

# Buat sample kecil untuk evaluasi model
sample_val = pd.DataFrame({
    'context': val_texts,
    'question': val_questions,
    'answer': val_answers
}).sample(20, random_state=42).reset_index(drop=True)

"""Penjelasan:

Kode ini digunakan membuat sampel kecil sebanyak 20 baris dari data validasi secara acak untuk evaluasi model. Sampel disusun dalam DataFrame baru dengan indeks yang sudah direset agar rapi dan konsisten.

## **VISUALISASI DATASET**
"""

# 1. Distribusi panjang konteks (jumlah kata dalam konteks)
df['context_length'] = df['context'].apply(lambda x: len(x.split()))

# Tampilkan statistik deskriptif angka pastinya
print("Statistik Panjang Konteks (Jumlah Kata):")
print(df['context_length'].describe())

# Buat histogram distribusi panjang konteks
plt.figure(figsize=(12, 6))
sns.histplot(df['context_length'], bins=30, kde=True)  # tambahkan kde=True untuk garis density
plt.title('Distribusi Panjang Konteks')
plt.xlabel('Jumlah Kata dalam Context')
plt.ylabel('Frekuensi')
plt.grid(True)
plt.show()

"""**Penjelasan:**

Kode ini menghitung panjang konteks di dataset dan menampilkan statistik deskriptif seperti rata-rata, deviasi standar, serta nilai minimum/maksimum. Histogram distribusi panjang konteks juga dibuat untuk menunjukkan frekuensi dan pola distribusi panjang konteks.

Hasil statistik panjang konteks (jumlah kata) menunjukkan informasi berikut:

- Jumlah data (count): 86,818 baris

- Rata-rata panjang konteks: 116 kata

- Deviasi standar (std): 48 kata (menunjukkan sebaran panjang konteks)

- Panjang konteks terpendek (min): 0 kata

- Panjang konteks kuartil pertama (25%): 86 kata

- Panjang konteks median (50%): 107 kata

- Panjang konteks kuartil ketiga (75%): 138 kata

- Panjang konteks terpanjang (max): 616 kata
"""

# 2. Distribusi panjang pertanyaan (jumlah kata dalam pertanyaan)
df['question_length'] = df['question'].apply(lambda x: len(x.split()))

# Tampilkan statistik deskriptif angka pastinya
print("Statistik Panjang Pertanyaan (Jumlah Kata):")
print(df['question_length'].describe())

# Buat histogram distribusi panjang pertanyaan
plt.figure(figsize=(12, 6))
sns.histplot(df['question_length'], bins=30, kde=True)  # tambahkan kde=True untuk garis density
plt.title('Distribusi Panjang Pertanyaan')
plt.xlabel('Jumlah Kata dalam Pertanyaan')
plt.ylabel('Frekuensi')
plt.grid(True)
plt.show()

"""**Penjelasan:**

Kode ini menghitung panjang pertanyaan di dataset dan menampilkan statistik deskriptif seperti rata-rata, deviasi standar, serta nilai minimum/maksimum. Histogram distribusi panjang pertanyaan juga dibuat untuk menunjukkan frekuensi dan pola distribusi panjang pertanyaan.

Hasil statistik panjang pertanyaan (jumlah kata) menunjukkan informasi berikut:

- Jumlah data (count): 86,818 baris

- Rata-rata panjang pertanyaan: 10 kata

- Deviasi standar (std): 3.55 kata (menunjukkan sebaran panjang pertanyaan)

- Panjang pertanyaan terpendek (min): 1 kata

- Panjang pertanyaan kuartil pertama (25%): 8 kata

- Panjang pertanyaan median (50%): 10 kata

- Panjang pertanyaan kuartil ketiga (75%): 12 kata

- Panjang pertanyaan terpanjang (max): 40 kata
"""

# 3. Distribusi panjang jawaban (jumlah kata dalam jawaban)
df['answer_length'] = df['answer'].apply(lambda x: len(x.split()))

# Tampilkan statistik deskriptif angka pastinya
print("Statistik Panjang Jawaban (Jumlah Kata):")
print(df['answer_length'].describe())

# Buat histogram distribusi panjang jawaban
plt.figure(figsize=(12, 6))
sns.histplot(df['answer_length'], bins=30, kde=True)  # tambahkan kde=True untuk garis density
plt.title('Distribusi Panjang Jawaban')
plt.xlabel('Jumlah Kata dalam Jawaban')
plt.ylabel('Frekuensi')
plt.grid(True)
plt.show()

"""**Penjelasan:**

Kode ini menghitung panjang jawaban di dataset dan menampilkan statistik deskriptif seperti rata-rata, deviasi standar, serta nilai minimum/maksimum. Histogram distribusi panjang jawaban juga dibuat untuk menunjukkan frekuensi dan pola distribusi panjang jawaban.

Hasil statistik panjang pertanyaan (jumlah kata) menunjukkan informasi berikut:

- Jumlah data (count): 86,818 baris

- Rata-rata panjang jawaban: 3.13 kata

- Deviasi standar (std): 3.34 kata (menunjukkan sebaran panjang jawaban)

- Panjang jawaban terpendek (min): 0 kata

- Panjang jawaban kuartil pertama (25%): 1 kata

- Panjang jawaban median (50%): 2 kata

- Panjang jawaban kuartil ketiga (75%): 3 kata

- Panjang jawaban terpanjang (max): 43 kata

# **Pemodelan Question and Answer**
"""

# Definisikan model-model
models = {
    "BERT": "bert-base-uncased",
    "DistilBERT": "distilbert-base-uncased",
    "RoBERTa": "deepset/roberta-base-squad2"
}

# Simpan hasil akurasi
results = {}

# Loop evaluasi tiap model
for model_name, model_checkpoint in models.items():
    print(f"\nEvaluating {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
    model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
    qa_pipe = pipeline('question-answering', model=model, tokenizer=tokenizer)

    correct = 0
    total = len(sample_val)

    for idx, row in sample_val.iterrows():
        context = row['context']
        question = row['question']
        true_answer = row['answer'].lower()

        pred = qa_pipe({'context': context, 'question': question})
        pred_answer = pred['answer'].lower()

        if true_answer in pred_answer or pred_answer in true_answer:
            correct += 1

    accuracy = correct / total
    results[model_name] = accuracy
    print(f"Accuracy {model_name}: {accuracy:.2f}")

# Tampilkan semua hasil
print("\n=== Akurasi Semua Model ===")
for name, acc in results.items():
    print(f"{name}: {acc:.2f}")

"""**Penjelasan:**

Kode ini digunakan untuk mengevaluasi akurasi tiga model machine learning dalam tugas question-answering (QA), yaitu BERT, DistilBERT, dan RoBERTa.

- Model dimuat menggunakan tokenizer dan model terlatih dari checkpoint yang relevan.

- Evaluasi dilakukan dengan membandingkan jawaban model dengan jawaban yang benar, dengan kesamaan meskipun ada variasi kecil dianggap sebagai jawaban yang benar.

- Akurasi dihitung sebagai rasio jawaban yang benar terhadap total data yang diuji dan hasilnya disimpan untuk masing-masing model.

- Perbandingan akurasi antar model memberikan gambaran tentang kinerja mereka dalam tugas QA.

=== Akurasi Semua Model ===

BERT: 0.05

DistilBERT: 0.05

RoBERTa: 0.95

# **Evaluasi Model**
"""

# Pilih model terbaik
best_model_name = max(results, key=results.get)
best_model_checkpoint = models[best_model_name]

print(f"\n=== Model Terbaik: {best_model_name} (Accuracy: {results[best_model_name]:.2f}) ===")

# Load model terbaik
tokenizer = AutoTokenizer.from_pretrained(best_model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(best_model_checkpoint)
qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)

# Fungsi Jawab Pertanyaan
def answer_question(context, question):
    response = qa_pipeline({'context': context, 'question': question})
    return response['answer']

"""**Penjelasan:**

Kode ini memilih model dengan akurasi tertinggi, lalu memuat tokenizer dan model terbaik. Setelah itu, dibuat pipeline QA untuk menjawab pertanyaan menggunakan model tersebut.

## **Deployment**
"""

import pandas as pd
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering

# Load model terbaik
tokenizer = AutoTokenizer.from_pretrained(best_model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(best_model_checkpoint)
qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)

# Load dataset (misalnya dari file CSV)
df = pd.read_csv("/content/drive/MyDrive/SQuAD/SQuAD_csv.csv") # Gantilah dengan path file dataset Anda

# Fungsi untuk mengambil konteks yang relevan dari dataset berdasarkan pertanyaan
def get_relevant_context(question):
    # Cari konteks yang paling relevan dari dataset
    # Dalam contoh ini, kita mencari konteks yang memiliki pertanyaan yang sama
    # Ini bisa diperluas dengan logika yang lebih kompleks (misalnya, mencari berdasarkan kemiripan teks)
    context = df.loc[df['question'].str.lower() == question.lower(), 'context']
    if not context.empty:
        return context.iloc[0]
    else:
        return "Maaf, saya tidak bisa menemukan konteks yang relevan untuk pertanyaan tersebut."

# Fungsi untuk menjawab pertanyaan menggunakan model
def answer_question(context, question):
    response = qa_pipeline({'context': context, 'question': question})
    return response['answer']

# Fungsi chatbot dengan konteks dari dataset
def chatbot_mode():
    print("="*50)
    print("WELCOME TO THE CHATBOT! Ketik 'exit' untuk keluar.")
    print("="*50)

    print("Dataset telah dimuat dan siap digunakan untuk menjawab pertanyaan!")

    print("\nSekarang kamu bisa bertanya! Ketik 'exit' kapan saja untuk keluar.")

    while True:
        question = input("Apa pertanyaan Anda? (atau ketik 'exit' untuk keluar):\n")
        if question.lower() == 'exit':
            print("Terima kasih telah menggunakan chatbot! 👋")
            break

        # Ambil konteks yang relevan berdasarkan pertanyaan
        context = get_relevant_context(question)

        # Menggunakan model untuk menjawab pertanyaan
        answer = answer_question(context, question)
        print(f"Jawaban: {answer}\n")

# Jalankan chatbot
chatbot_mode()

"""
Penjelasan:

Kode ini membuat chatbot berbasis question answering yang menggunakan dataset CSV berisi konteks dan pertanyaan. Saat pengguna mengajukan pertanyaan, fungsi get_relevant_context() mencari konteks paling relevan dari dataset. Model yang telah dilatih sebelumnya, dimuat dengan tokenizer dan pipeline dari pustaka transformers, lalu digunakan untuk menjawab pertanyaan berdasarkan konteks tersebut. Interaksi berlangsung lewat fungsi chatbot_mode(), dan pengguna bisa keluar kapan saja dengan mengetik "exit"."""